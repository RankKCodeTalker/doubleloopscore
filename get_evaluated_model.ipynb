{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import io\n",
    "evalset = io.jsonload(\"data/eval.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpt3.5、gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子密钥余额不足，请及时充值或调整用量上限，当您在使用 gpt-4 系列模型时，请确保该密钥余额大于 327680 tokens.\n",
      "choices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/kirin/.conda/envs/wkyc/lib/python3.10/site-packages/openai/openai_object.py\", line 59, in __getattr__\n",
      "    return self[k]\n",
      "KeyError: 'choices'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3174548/2772328579.py\", line 41, in <module>\n",
      "    model_response = call_gpt(question, 'gpt-4', api_key, api_base, max_tokens=512)\n",
      "  File \"/tmp/ipykernel_3174548/2772328579.py\", line 27, in call_gpt\n",
      "    return completion.choices[0].message.content\n",
      "  File \"/home/kirin/.conda/envs/wkyc/lib/python3.10/site-packages/openai/openai_object.py\", line 61, in __getattr__\n",
      "    raise AttributeError(*err.args)\n",
      "AttributeError: choices\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb 单元格 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         i \u001b[39m=\u001b[39m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39mlen\u001b[39m(api_keys)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m result\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: question,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m: model_response\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m })\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m io\u001b[39m.\u001b[39mjsondump(result, topath)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kirin/wkyc/doubleloopscore/get_evaluated_model.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m i \u001b[39m=\u001b[39m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39mlen\u001b[39m(api_keys)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_response' is not defined"
     ]
    }
   ],
   "source": [
    "from myutils import net, gpt\n",
    "import os\n",
    "import openai\n",
    "net.set_proxy()\n",
    "api_base = \"https://api.nextweb.fun/openai/v1\"\n",
    "api_keys = [\n",
    "]\n",
    "from typing import Literal\n",
    "optional_models = Literal['gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-4', 'gpt-4-32k']\n",
    "def call_gpt(prompt: str, model: optional_models='gpt-3.5-turbo', api_key: str=None, api_base: str=None, **kwargs) -> str:\n",
    "    if api_key is not None:\n",
    "        openai.api_key = api_key\n",
    "    if api_base is not None:\n",
    "        openai.api_base = api_base\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.01,\n",
    "        **kwargs\n",
    "    )\n",
    "    print(completion)\n",
    "    return completion.choices[0].message.content\n",
    "topath = \"evaluated_models/gpt4.json\"\n",
    "if os.path.exists(topath):\n",
    "    result = io.jsonload(topath)\n",
    "else:\n",
    "    result = []\n",
    "start = len(result)\n",
    "i = 0\n",
    "for unit in evalset[start:]:\n",
    "    question = unit['question']\n",
    "    lock = True\n",
    "    while lock:\n",
    "        try:\n",
    "            api_key = api_keys[i]\n",
    "            model_response = call_gpt(question, 'gpt-4', api_key, api_base, max_tokens=512)\n",
    "            lock = False\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            i = (i+1)%len(api_keys)\n",
    "            break\n",
    "    result.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": model_response\n",
    "    })\n",
    "    io.jsondump(result, topath)\n",
    "    i = (i+1)%len(api_keys)\n",
    "net.unset_proxy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatglm-6b、chatglm2-6b、qwen7b、baichuan-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [17:45<00:00,  8.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from src.referee_model import call_glm\n",
    "import os\n",
    "topath = \"evaluated_models/llama2-7bchat.json\"\n",
    "if os.path.exists(topath):\n",
    "    result = io.jsonload(topath)\n",
    "else:\n",
    "    result = []\n",
    "start = len(result)\n",
    "for unit in tqdm(evalset[start:]):\n",
    "    question = unit['question']\n",
    "    model_response = call_glm(question)\n",
    "    result.append({\n",
    "        \"question\": question,\n",
    "        \"answer\": model_response\n",
    "    })\n",
    "    io.jsondump(result, topath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
